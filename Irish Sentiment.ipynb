{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Jamil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Jamil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gensim\n",
    "import nltk\n",
    "import pickle\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "nltk.download('stopwords')\n",
    "from tpot import TPOTClassifier\n",
    "from datetime import datetime\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import SCORERS, accuracy_score, f1_score\n",
    "import h2o\n",
    "from h2o.automl import H2OAutoML\n",
    "\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "DATE_FORMAT = \"%Y-%m-%d\"\n",
    "MIN_DATE = datetime.strptime('2009-07-07', DATE_FORMAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_terms(terms_file):\n",
    "    term_dict = {}\n",
    "    \n",
    "    for line in terms_file:\n",
    "        key, word = line.split(\",\")\n",
    "        term_dict[int(key)] = word.split(\"\\n\")[0]\n",
    "        \n",
    "    return term_dict\n",
    "\n",
    "def get_labels(labels_file):\n",
    "    curr_article = -1\n",
    "    labels_votes = []\n",
    "    article_dict = {}\n",
    "\n",
    "    for line in labels_file:\n",
    "        split_line = line.split(\",\")\n",
    "\n",
    "        if int(split_line[0]) != curr_article:\n",
    "            curr_article = int(split_line[0])\n",
    "            article_dict[curr_article] = True\n",
    "            labels_votes.append([0, 0, 0]) # index 0 = negative, 1 = irrelevant, 2 = positive\n",
    "\n",
    "        if 'negative' in split_line[2]:\n",
    "            labels_votes[-1][0] += 1\n",
    "        elif 'irrelevant' in split_line[2]:\n",
    "            labels_votes[-1][1] += 1\n",
    "        else:\n",
    "            labels_votes[-1][2] += 1\n",
    "\n",
    "    labels = []\n",
    "\n",
    "    for vote_counts in labels_votes:\n",
    "        labels.append(np.argmax(vote_counts))\n",
    "        \n",
    "    return labels, article_dict\n",
    "\n",
    "def create_bow_and_features(word_list, term_dict):\n",
    "    word_list = word_list.split(\",\")\n",
    "    \n",
    "    doc = []\n",
    "    \n",
    "    for i, entry in enumerate(word_list):\n",
    "        if i > 2:\n",
    "            word, freq = entry.split(\":\")\n",
    "\n",
    "            if term_dict[int(word)] not in stop_words:\n",
    "                doc.append((int(word), int(freq)))\n",
    "            \n",
    "    curr_date = datetime.strptime(word_list[1], DATE_FORMAT)\n",
    "    delta = curr_date - MIN_DATE\n",
    "    return doc, word_list[0], delta.days, word_list[2]\n",
    "\n",
    "def preprocess_datasets(articles_dir, terms_dir, annotations_dir, preload_lda=False):\n",
    "    terms_file = open(terms_dir)\n",
    "    term_dict = get_terms(terms_file)\n",
    "    \n",
    "    dataset = []\n",
    "    features = []\n",
    "    articles_file = open(articles_dir)\n",
    "\n",
    "    for line in articles_file:\n",
    "        article, article_id, date_in_days, provider = create_bow_and_features(line, term_dict)\n",
    "        dataset.append(article)\n",
    "        features.append([article_id, provider, date_in_days])\n",
    "\n",
    "    features = pd.DataFrame(features)\n",
    "    \n",
    "    ldamodel = None\n",
    "    num_topics = 30\n",
    "    \n",
    "    if preload_lda is False:\n",
    "        ldamodel = gensim.models.ldamodel.LdaModel(dataset, num_topics = num_topics, passes=15)\n",
    "        ldamodel.save('lda/irish_sentiment/model5.gensim')\n",
    "    else:\n",
    "        ldamodel =  gensim.models.ldamodel.LdaModel.load('lda/irish_sentiment/model5.gensim')\n",
    "        \n",
    "    topic_features = []\n",
    "\n",
    "    for doc in dataset:\n",
    "        row = [0.0 for i in range(num_topics)]\n",
    "\n",
    "        for topic, prob in ldamodel[doc]:\n",
    "            row[topic] = prob\n",
    "\n",
    "        topic_features.append(row)\n",
    "\n",
    "    topic_features = pd.DataFrame(topic_features)\n",
    "    \n",
    "    features = pd.concat([features, topic_features], axis=1)\n",
    "\n",
    "    columns = ['id', 'publisher', 'date_in_days']\n",
    "    topic_columns = [i for i in range(num_topics)]\n",
    "    columns = np.concatenate([columns, topic_columns])\n",
    "    features.columns = columns\n",
    "    \n",
    "    labels_file = open(annotations_dir)\n",
    "    labels, article_dict = get_labels(labels_file)\n",
    "    \n",
    "    drop_list = []\n",
    "\n",
    "    for index, row, in features.iterrows():\n",
    "        if int(row['id']) not in article_dict:\n",
    "            drop_list.append(index)\n",
    "            \n",
    "    features = features.drop(drop_list, axis=0)\n",
    "    features = features.drop('id', axis=1)\n",
    "    features = pd.get_dummies(features)\n",
    "    features = features.reset_index()\n",
    "    features = features.drop('index', axis=1)\n",
    "    \n",
    "    return features, np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_dir = r'datasets\\irish_sentiment\\sentiment_all_terms.csv'\n",
    "articles_dir = r'datasets\\irish_sentiment\\sentiment_all_articles.csv'\n",
    "annotations_dir = r'datasets\\irish_sentiment\\sentiment_all_annotations.csv'\n",
    "\n",
    "features, labels = preprocess_datasets(articles_dir, terms_dir, annotations_dir, preload_lda=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: xgboost.XGBClassifier is not available and will not be used by TPOT.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d16680f8567049d3aa566a9e20d11ec8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Optimization Progress', style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: 0.6453028798411121\n",
      "Generation 2 - Current best internal CV score: 0.6453028798411121\n",
      "Generation 3 - Current best internal CV score: 0.6475727053482763\n",
      "Generation 4 - Current best internal CV score: 0.6475727053482763\n",
      "Generation 5 - Current best internal CV score: 0.6483359341750603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:stopit:Code block execution exceeded 2 seconds timeout\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\stopit\\utils.py\", line 145, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\tpot\\decorators.py\", line 57, in time_limited_call\n",
      "    func(*args)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\sklearn\\pipeline.py\", line 330, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\sklearn\\pipeline.py\", line 296, in _fit\n",
      "    **fit_params_steps[name])\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\sklearn\\pipeline.py\", line 740, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\sklearn\\base.py\", line 693, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\sklearn\\feature_selection\\_rfe.py\", line 151, in fit\n",
      "    return self._fit(X, y)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\sklearn\\feature_selection\\_rfe.py\", line 196, in _fit\n",
      "    estimator.fit(X[:, features], y)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 378, in fit\n",
      "    for i in range(n_more_estimators)]\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 378, in <listcomp>\n",
      "    for i in range(n_more_estimators)]\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\sklearn\\ensemble\\_base.py\", line 151, in _make_estimator\n",
      "    estimator = clone(self.base_estimator_)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 73, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\sklearn\\base.py\", line 85, in clone\n",
      "    new_object_params = estimator.get_params(deep=False)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\sklearn\\base.py\", line 205, in get_params\n",
      "    for key in self._get_param_names():\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\sklearn\\base.py\", line 174, in _get_param_names\n",
      "    init_signature = inspect.signature(init)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\inspect.py\", line 3083, in signature\n",
      "    return Signature.from_callable(obj, follow_wrapped=follow_wrapped)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\inspect.py\", line 2833, in from_callable\n",
      "    follow_wrapper_chains=follow_wrapped)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\inspect.py\", line 2284, in _signature_from_callable\n",
      "    return _signature_from_function(sigcls, obj)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\inspect.py\", line 2179, in _signature_from_function\n",
      "    default=default))\n",
      "stopit.utils.TimeoutException\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 6 - Current best internal CV score: 0.6505773868633848\n",
      "Generation 7 - Current best internal CV score: 0.6543367853596254\n",
      "Generation 8 - Current best internal CV score: 0.6543367853596254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:stopit:Code block execution exceeded 2 seconds timeout\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\stopit\\utils.py\", line 145, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\tpot\\decorators.py\", line 57, in time_limited_call\n",
      "    func(*args)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\sklearn\\pipeline.py\", line 335, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\joblib\\parallel.py\", line 1032, in __call__\n",
      "    while self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\joblib\\parallel.py\", line 847, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\joblib\\parallel.py\", line 765, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 206, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 570, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\joblib\\parallel.py\", line 253, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\joblib\\parallel.py\", line 253, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 894, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 375, in fit\n",
      "    builder.build(self.tree_, X, y, sample_weight, X_idx_sorted)\n",
      "stopit.utils.TimeoutException\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 9 - Current best internal CV score: 0.6543367853596254\n",
      "Generation 10 - Current best internal CV score: 0.6543367853596254\n",
      "Generation 11 - Current best internal CV score: 0.6543367853596254\n",
      "Generation 12 - Current best internal CV score: 0.6551028514682934\n",
      "Generation 13 - Current best internal CV score: 0.6565952617392538\n",
      "Generation 14 - Current best internal CV score: 0.6588509008369982\n",
      "Generation 15 - Current best internal CV score: 0.6588509008369982\n",
      "Generation 16 - Current best internal CV score: 0.6588707618101859\n",
      "Generation 17 - Current best internal CV score: 0.6588707618101859\n",
      "Generation 18 - Current best internal CV score: 0.6596056178181303\n",
      "Generation 19 - Current best internal CV score: 0.6596056178181303\n",
      "Generation 20 - Current best internal CV score: 0.6596056178181303\n",
      "Generation 21 - Current best internal CV score: 0.6596056178181303\n",
      "Generation 22 - Current best internal CV score: 0.6596084551000142\n",
      "Generation 23 - Current best internal CV score: 0.6596084551000142\n",
      "Generation 24 - Current best internal CV score: 0.6603716839267981\n",
      "Generation 25 - Current best internal CV score: 0.6603716839267981\n",
      "Generation 26 - Current best internal CV score: 0.6603716839267981\n",
      "Generation 27 - Current best internal CV score: 0.6603716839267981\n",
      "Generation 28 - Current best internal CV score: 0.6603716839267981\n",
      "Generation 29 - Current best internal CV score: 0.6618584196339906\n",
      "\n",
      "60.02 minutes have elapsed. TPOT will close down.\n",
      "TPOT closed during evaluation in one generation.\n",
      "WARNING: TPOT may not provide a good pipeline if TPOT is stopped/interrupted in a early generation.\n",
      "\n",
      "\n",
      "TPOT closed prematurely. Will use the current best pipeline.\n",
      "\n",
      "Best pipeline: LinearSVC(BernoulliNB(MultinomialNB(MinMaxScaler(BernoulliNB(input_matrix, alpha=10.0, fit_prior=False)), alpha=100.0, fit_prior=True), alpha=1.0, fit_prior=True), C=15.0, dual=False, loss=squared_hinge, penalty=l1, tol=0.1)\n",
      "Warning: xgboost.XGBClassifier is not available and will not be used by TPOT.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d527b74d3a41496b8cd0aec823f0792b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Optimization Progress', style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:stopit:Code block execution exceeded 2 seconds timeout\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\stopit\\utils.py\", line 145, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\tpot\\decorators.py\", line 57, in time_limited_call\n",
      "    func(*args)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\sklearn\\pipeline.py\", line 330, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\sklearn\\pipeline.py\", line 296, in _fit\n",
      "    **fit_params_steps[name])\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\sklearn\\pipeline.py\", line 740, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\sklearn\\base.py\", line 693, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\sklearn\\feature_selection\\_rfe.py\", line 151, in fit\n",
      "    return self._fit(X, y)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\sklearn\\feature_selection\\_rfe.py\", line 196, in _fit\n",
      "    estimator.fit(X[:, features], y)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 378, in fit\n",
      "    for i in range(n_more_estimators)]\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 378, in <listcomp>\n",
      "    for i in range(n_more_estimators)]\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\sklearn\\ensemble\\_base.py\", line 151, in _make_estimator\n",
      "    estimator = clone(self.base_estimator_)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 73, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\sklearn\\base.py\", line 87, in clone\n",
      "    new_object_params[name] = clone(param, safe=False)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 73, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\sklearn\\base.py\", line 69, in clone\n",
      "    elif not hasattr(estimator, 'get_params') or isinstance(estimator, type):\n",
      "stopit.utils.TimeoutException\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: 0.6384934033196198\n",
      "Generation 2 - Current best internal CV score: 0.6384934033196198\n",
      "Generation 3 - Current best internal CV score: 0.650568875017733\n",
      "Generation 4 - Current best internal CV score: 0.650568875017733\n",
      "Generation 5 - Current best internal CV score: 0.650568875017733\n",
      "Generation 6 - Current best internal CV score: 0.650568875017733\n",
      "Generation 7 - Current best internal CV score: 0.650568875017733\n",
      "Generation 8 - Current best internal CV score: 0.65126968364307\n",
      "Generation 9 - Current best internal CV score: 0.65126968364307\n",
      "Generation 10 - Current best internal CV score: 0.65126968364307\n",
      "Generation 11 - Current best internal CV score: 0.65126968364307\n",
      "Generation 12 - Current best internal CV score: 0.6520783089799972\n",
      "Generation 13 - Current best internal CV score: 0.6520783089799972\n",
      "Generation 14 - Current best internal CV score: 0.6550574549581502\n",
      "Generation 15 - Current best internal CV score: 0.6550574549581502\n",
      "Generation 16 - Current best internal CV score: 0.6618527450702227\n",
      "Generation 17 - Current best internal CV score: 0.6618527450702227\n",
      "Generation 18 - Current best internal CV score: 0.6618527450702227\n",
      "Generation 19 - Current best internal CV score: 0.6618527450702227\n",
      "Generation 20 - Current best internal CV score: 0.6618527450702227\n",
      "Generation 21 - Current best internal CV score: 0.6618527450702227\n",
      "Generation 22 - Current best internal CV score: 0.6618527450702227\n",
      "Generation 23 - Current best internal CV score: 0.6618527450702227\n",
      "Generation 24 - Current best internal CV score: 0.6618527450702227\n",
      "Generation 25 - Current best internal CV score: 0.6618527450702227\n",
      "\n",
      "60.06 minutes have elapsed. TPOT will close down.\n",
      "TPOT closed during evaluation in one generation.\n",
      "WARNING: TPOT may not provide a good pipeline if TPOT is stopped/interrupted in a early generation.\n",
      "\n",
      "\n",
      "TPOT closed prematurely. Will use the current best pipeline.\n",
      "\n",
      "Best pipeline: LinearSVC(Normalizer(StandardScaler(MaxAbsScaler(FastICA(input_matrix, tol=0.8500000000000001))), norm=max), C=0.01, dual=False, loss=squared_hinge, penalty=l2, tol=0.1)\n",
      "Warning: xgboost.XGBClassifier is not available and will not be used by TPOT.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fd4dd3143284d4aa2ae9b8b34383c53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Optimization Progress', style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: 0.6490282309547454\n",
      "Generation 2 - Current best internal CV score: 0.6550432685487303\n",
      "Generation 3 - Current best internal CV score: 0.6588282025819265\n",
      "Generation 4 - Current best internal CV score: 0.6595772449992906\n",
      "Generation 5 - Current best internal CV score: 0.6595772449992906\n",
      "Generation 6 - Current best internal CV score: 0.6595772449992906\n",
      "Generation 7 - Current best internal CV score: 0.6611008653709746\n",
      "Generation 8 - Current best internal CV score: 0.6625989502057029\n",
      "Generation 9 - Current best internal CV score: 0.6625989502057029\n",
      "Generation 10 - Current best internal CV score: 0.6633309689317635\n",
      "Generation 11 - Current best internal CV score: 0.6633309689317635\n",
      "Generation 12 - Current best internal CV score: 0.6641055468860831\n",
      "Generation 13 - Current best internal CV score: 0.6641055468860831\n",
      "Generation 14 - Current best internal CV score: 0.6648574265853312\n",
      "Generation 15 - Current best internal CV score: 0.6671244148106116\n",
      "Generation 16 - Current best internal CV score: 0.6671244148106116\n",
      "Generation 17 - Current best internal CV score: 0.6701376081713718\n",
      "Generation 18 - Current best internal CV score: 0.6701376081713718\n",
      "Generation 19 - Current best internal CV score: 0.6701376081713718\n",
      "Generation 20 - Current best internal CV score: 0.6701376081713718\n",
      "Generation 21 - Current best internal CV score: 0.6701376081713718\n",
      "Generation 22 - Current best internal CV score: 0.6701376081713718\n",
      "Generation 23 - Current best internal CV score: 0.6701376081713718\n",
      "Generation 24 - Current best internal CV score: 0.6701376081713718\n",
      "Generation 25 - Current best internal CV score: 0.6701376081713718\n",
      "Generation 26 - Current best internal CV score: 0.6701376081713718\n",
      "Generation 27 - Current best internal CV score: 0.6701376081713718\n",
      "Generation 28 - Current best internal CV score: 0.6701376081713718\n",
      "Generation 29 - Current best internal CV score: 0.6701376081713718\n",
      "Generation 30 - Current best internal CV score: 0.6701376081713718\n",
      "Generation 31 - Current best internal CV score: 0.6701376081713718\n",
      "Generation 32 - Current best internal CV score: 0.6701376081713718\n",
      "Generation 33 - Current best internal CV score: 0.6701376081713718\n",
      "Generation 34 - Current best internal CV score: 0.6708667896155484\n",
      "Generation 35 - Current best internal CV score: 0.6723847354234642\n",
      "Generation 36 - Current best internal CV score: 0.6723847354234642\n",
      "\n",
      "60.01 minutes have elapsed. TPOT will close down.\n",
      "TPOT closed during evaluation in one generation.\n",
      "WARNING: TPOT may not provide a good pipeline if TPOT is stopped/interrupted in a early generation.\n",
      "\n",
      "\n",
      "TPOT closed prematurely. Will use the current best pipeline.\n",
      "\n",
      "Best pipeline: LinearSVC(BernoulliNB(LinearSVC(input_matrix, C=0.1, dual=True, loss=hinge, penalty=l2, tol=1e-05), alpha=0.001, fit_prior=True), C=15.0, dual=False, loss=squared_hinge, penalty=l2, tol=0.0001)\n",
      "Warning: xgboost.XGBClassifier is not available and will not be used by TPOT.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da99128505b247c9b52d61e0947cbb6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Optimization Progress', style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: 0.6324840402894027\n",
      "Generation 2 - Current best internal CV score: 0.6324840402894027\n",
      "Generation 3 - Current best internal CV score: 0.6438076322882679\n",
      "Generation 4 - Current best internal CV score: 0.6438076322882679\n",
      "Generation 5 - Current best internal CV score: 0.6438076322882679\n",
      "Generation 6 - Current best internal CV score: 0.6460519222584764\n",
      "Generation 7 - Current best internal CV score: 0.6460519222584764\n",
      "Generation 8 - Current best internal CV score: 0.6460519222584764\n",
      "Generation 9 - Current best internal CV score: 0.6468265002127962\n",
      "Generation 10 - Current best internal CV score: 0.6468265002127962\n",
      "Generation 11 - Current best internal CV score: 0.6468265002127962\n",
      "Generation 12 - Current best internal CV score: 0.6468265002127962\n",
      "Generation 13 - Current best internal CV score: 0.6490764647467726\n",
      "Generation 14 - Current best internal CV score: 0.6490764647467726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:stopit:Code block execution exceeded 2 seconds timeout\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\stopit\\utils.py\", line 145, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\tpot\\decorators.py\", line 57, in time_limited_call\n",
      "    func(*args)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\sklearn\\pipeline.py\", line 330, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\sklearn\\pipeline.py\", line 296, in _fit\n",
      "    **fit_params_steps[name])\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\sklearn\\pipeline.py\", line 740, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\tpot\\builtins\\one_hot_encoder.py\", line 399, in fit_transform\n",
      "    copy=True\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\tpot\\builtins\\one_hot_encoder.py\", line 128, in _transform_selected\n",
      "    X_sel = transform(X_sel)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\tpot\\builtins\\one_hot_encoder.py\", line 303, in _fit_transform\n",
      "    unique = np.unique(X[:, column])\n",
      "  File \"<__array_function__ internals>\", line 6, in unique\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\numpy\\lib\\arraysetops.py\", line 263, in unique\n",
      "    ret = _unique1d(ar, return_index, return_inverse, return_counts)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\numpy\\lib\\arraysetops.py\", line 315, in _unique1d\n",
      "    mask[1:] = aux[1:] != aux[:-1]\n",
      "stopit.utils.TimeoutException\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 15 - Current best internal CV score: 0.6490764647467726\n",
      "Generation 16 - Current best internal CV score: 0.6490764647467726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:stopit:Code block execution exceeded 2 seconds timeout\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\stopit\\utils.py\", line 145, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\tpot\\decorators.py\", line 57, in time_limited_call\n",
      "    func(*args)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\sklearn\\pipeline.py\", line 330, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\sklearn\\pipeline.py\", line 296, in _fit\n",
      "    **fit_params_steps[name])\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\sklearn\\pipeline.py\", line 740, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\sklearn\\base.py\", line 693, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\sklearn\\feature_selection\\_rfe.py\", line 151, in fit\n",
      "    return self._fit(X, y)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\sklearn\\feature_selection\\_rfe.py\", line 196, in _fit\n",
      "    estimator.fit(X[:, features], y)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\joblib\\parallel.py\", line 1032, in __call__\n",
      "    while self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\joblib\\parallel.py\", line 847, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\joblib\\parallel.py\", line 765, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 206, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 570, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\joblib\\parallel.py\", line 253, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\joblib\\parallel.py\", line 253, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 170, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 894, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 375, in fit\n",
      "    builder.build(self.tree_, X, y, sample_weight, X_idx_sorted)\n",
      "stopit.utils.TimeoutException\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 17 - Current best internal CV score: 0.6490764647467726\n",
      "Generation 18 - Current best internal CV score: 0.6490764647467726\n",
      "Generation 19 - Current best internal CV score: 0.6490764647467726\n",
      "Generation 20 - Current best internal CV score: 0.6490906511561924\n",
      "Generation 21 - Current best internal CV score: 0.6566122854305576\n",
      "Generation 22 - Current best internal CV score: 0.6566122854305576\n",
      "Generation 23 - Current best internal CV score: 0.6566122854305576\n",
      "Generation 24 - Current best internal CV score: 0.6566122854305576\n",
      "Generation 25 - Current best internal CV score: 0.6566122854305576\n",
      "Generation 26 - Current best internal CV score: 0.6566122854305576\n",
      "Generation 27 - Current best internal CV score: 0.6566122854305576\n",
      "Generation 28 - Current best internal CV score: 0.6566122854305576\n",
      "Generation 29 - Current best internal CV score: 0.6566122854305576\n",
      "Generation 30 - Current best internal CV score: 0.6566122854305576\n",
      "\n",
      "60.01 minutes have elapsed. TPOT will close down.\n",
      "TPOT closed during evaluation in one generation.\n",
      "WARNING: TPOT may not provide a good pipeline if TPOT is stopped/interrupted in a early generation.\n",
      "\n",
      "\n",
      "TPOT closed prematurely. Will use the current best pipeline.\n",
      "\n",
      "Best pipeline: KNeighborsClassifier(ExtraTreesClassifier(FastICA(input_matrix, tol=0.6000000000000001), bootstrap=True, criterion=entropy, max_features=0.8500000000000001, min_samples_leaf=11, min_samples_split=5, n_estimators=100), n_neighbors=12, p=1, weights=distance)\n",
      "Warning: xgboost.XGBClassifier is not available and will not be used by TPOT.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43194d5d28e14adc91e2bc20213b6f34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Optimization Progress', style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: 0.6324982266988226\n",
      "Generation 2 - Current best internal CV score: 0.6362689743225989\n",
      "Generation 3 - Current best internal CV score: 0.6362689743225989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:stopit:Code block execution exceeded 2 seconds timeout\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\stopit\\utils.py\", line 145, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\tpot\\decorators.py\", line 57, in time_limited_call\n",
      "    func(*args)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\sklearn\\pipeline.py\", line 335, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1415, in fit\n",
      "    for class_, warm_start_coef_ in zip(classes_, warm_start_coef))\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\joblib\\parallel.py\", line 1029, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\joblib\\parallel.py\", line 847, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\joblib\\parallel.py\", line 765, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 206, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 570, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\joblib\\parallel.py\", line 253, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\joblib\\parallel.py\", line 253, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 758, in _logistic_regression_path\n",
      "    options={\"iprint\": iprint, \"gtol\": tol, \"maxiter\": max_iter}\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\scipy\\optimize\\_minimize.py\", line 600, in minimize\n",
      "    callback=callback, **options)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\scipy\\optimize\\lbfgsb.py\", line 335, in _minimize_lbfgsb\n",
      "    f, g = func_and_grad(x)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\scipy\\optimize\\lbfgsb.py\", line 285, in func_and_grad\n",
      "    f = fun(x, *args)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\scipy\\optimize\\optimize.py\", line 326, in function_wrapper\n",
      "    return function(*(wrapper_args + args))\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\scipy\\optimize\\optimize.py\", line 64, in __call__\n",
      "    fg = self.fun(x, *args)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 733, in func\n",
      "    def func(x, *args): return _multinomial_loss_grad(x, *args)[0:2]\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 347, in _multinomial_loss_grad\n",
      "    loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 299, in _multinomial_loss\n",
      "    loss += 0.5 * alpha * squared_norm(w)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\sklearn\\utils\\extmath.py\", line 41, in squared_norm\n",
      "    x = np.ravel(x, order='K')\n",
      "  File \"<__array_function__ internals>\", line 6, in ravel\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\numpy\\core\\fromnumeric.py\", line 1797, in ravel\n",
      "    return asanyarray(a).ravel(order=order)\n",
      "stopit.utils.TimeoutException\n",
      "WARNING:stopit:Code block execution exceeded 2 seconds timeout\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\stopit\\utils.py\", line 145, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\tpot\\decorators.py\", line 57, in time_limited_call\n",
      "    func(*args)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\sklearn\\pipeline.py\", line 335, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1415, in fit\n",
      "    for class_, warm_start_coef_ in zip(classes_, warm_start_coef))\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\joblib\\parallel.py\", line 1029, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\joblib\\parallel.py\", line 847, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\joblib\\parallel.py\", line 765, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 206, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 570, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\joblib\\parallel.py\", line 253, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\joblib\\parallel.py\", line 253, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 758, in _logistic_regression_path\n",
      "    options={\"iprint\": iprint, \"gtol\": tol, \"maxiter\": max_iter}\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\scipy\\optimize\\_minimize.py\", line 600, in minimize\n",
      "    callback=callback, **options)\n",
      "  File \"C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\scipy\\optimize\\lbfgsb.py\", line 328, in _minimize_lbfgsb\n",
      "    isave, dsave, maxls)\n",
      "stopit.utils.TimeoutException\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 4 - Current best internal CV score: 0.6362689743225989\n",
      "Generation 5 - Current best internal CV score: 0.6407830897999716\n",
      "Generation 6 - Current best internal CV score: 0.6407830897999716\n",
      "Generation 7 - Current best internal CV score: 0.6452943679954604\n",
      "Generation 8 - Current best internal CV score: 0.6452943679954604\n",
      "Generation 9 - Current best internal CV score: 0.6453000425592282\n",
      "Generation 10 - Current best internal CV score: 0.6453000425592282\n",
      "Generation 11 - Current best internal CV score: 0.6453000425592282\n",
      "Generation 12 - Current best internal CV score: 0.6460661086678962\n",
      "Generation 13 - Current best internal CV score: 0.6460661086678962\n",
      "Generation 14 - Current best internal CV score: 0.6468009646758406\n",
      "Generation 15 - Current best internal CV score: 0.6520726344162292\n",
      "Generation 16 - Current best internal CV score: 0.6520726344162292\n",
      "Generation 17 - Current best internal CV score: 0.6520726344162292\n",
      "Generation 18 - Current best internal CV score: 0.6520726344162292\n",
      "Generation 19 - Current best internal CV score: 0.6520726344162292\n",
      "Generation 20 - Current best internal CV score: 0.6520726344162292\n",
      "Generation 21 - Current best internal CV score: 0.6535962547879132\n",
      "Generation 22 - Current best internal CV score: 0.6551028514682933\n",
      "Generation 23 - Current best internal CV score: 0.6551028514682933\n",
      "Generation 24 - Current best internal CV score: 0.6551028514682933\n",
      "Generation 25 - Current best internal CV score: 0.6551028514682933\n",
      "Generation 26 - Current best internal CV score: 0.6551028514682933\n",
      "Generation 27 - Current best internal CV score: 0.6558604057313094\n",
      "Generation 28 - Current best internal CV score: 0.6558604057313094\n",
      "Generation 29 - Current best internal CV score: 0.6558604057313094\n",
      "Generation 30 - Current best internal CV score: 0.6558604057313094\n",
      "Generation 31 - Current best internal CV score: 0.6558604057313094\n",
      "Generation 32 - Current best internal CV score: 0.6558604057313094\n",
      "Generation 33 - Current best internal CV score: 0.6558604057313094\n",
      "Generation 34 - Current best internal CV score: 0.6581103702652858\n",
      "Generation 35 - Current best internal CV score: 0.6581103702652858\n",
      "Generation 36 - Current best internal CV score: 0.6581103702652858\n",
      "Generation 37 - Current best internal CV score: 0.6581103702652858\n",
      "\n",
      "60.06 minutes have elapsed. TPOT will close down.\n",
      "TPOT closed during evaluation in one generation.\n",
      "WARNING: TPOT may not provide a good pipeline if TPOT is stopped/interrupted in a early generation.\n",
      "\n",
      "\n",
      "TPOT closed prematurely. Will use the current best pipeline.\n",
      "\n",
      "Best pipeline: LinearSVC(BernoulliNB(LinearSVC(input_matrix, C=25.0, dual=True, loss=squared_hinge, penalty=l2, tol=0.01), alpha=0.01, fit_prior=False), C=0.5, dual=False, loss=squared_hinge, penalty=l2, tol=0.001)\n"
     ]
    }
   ],
   "source": [
    "# TPOT Testing\n",
    "kf = KFold(5, shuffle=True, random_state=42)\n",
    "\n",
    "acc = []\n",
    "f1 = []\n",
    "\n",
    "for train_ind, val_ind in kf.split(features, labels):\n",
    "    X_train, y_train = features.iloc[train_ind], labels[train_ind]\n",
    "    X_val, y_val = features.iloc[val_ind], labels[val_ind]\n",
    "    \n",
    "    tpot = TPOTClassifier(max_time_mins=60, verbosity=2)\n",
    "    tpot.fit(X_train, y_train)\n",
    "    clf = tpot.fitted_pipeline_\n",
    "    \n",
    "    \n",
    "    acc.append(SCORERS['accuracy'](clf, X_val, y_val))\n",
    "    f1.append(SCORERS['f1_macro'](clf, X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6475903614457831, 0.6234939759036144, 0.6325301204819277, 0.6807228915662651, 0.6746987951807228]\n",
      "[0.5993773785237396, 0.5375023527197441, 0.6079613095238096, 0.6548687331913934, 0.6408440686855803]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(acc)\n",
    "print(f1)\n",
    "print(neg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321 ..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "; OpenJDK 64-Bit Server VM Zulu11.2+3 (build 11.0.1+13-LTS, mixed mode)\n",
      "  Starting server from C:\\Users\\Jamil\\Anaconda3\\envs\\NLP\\lib\\site-packages\\h2o\\backend\\bin\\h2o.jar\n",
      "  Ice root: C:\\Users\\Jamil\\AppData\\Local\\Temp\\tmpt2jlrru_\n",
      "  JVM stdout: C:\\Users\\Jamil\\AppData\\Local\\Temp\\tmpt2jlrru_\\h2o_Jamil_started_from_python.out\n",
      "  JVM stderr: C:\\Users\\Jamil\\AppData\\Local\\Temp\\tmpt2jlrru_\\h2o_Jamil_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54321\n",
      "Connecting to H2O server at http://127.0.0.1:54321 ... successful.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O_cluster_uptime:</td>\n",
       "<td>01 secs</td></tr>\n",
       "<tr><td>H2O_cluster_timezone:</td>\n",
       "<td>Australia/Sydney</td></tr>\n",
       "<tr><td>H2O_data_parsing_timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O_cluster_version:</td>\n",
       "<td>3.30.0.3</td></tr>\n",
       "<tr><td>H2O_cluster_version_age:</td>\n",
       "<td>6 days </td></tr>\n",
       "<tr><td>H2O_cluster_name:</td>\n",
       "<td>H2O_from_python_Jamil_5f9k98</td></tr>\n",
       "<tr><td>H2O_cluster_total_nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O_cluster_free_memory:</td>\n",
       "<td>1.984 Gb</td></tr>\n",
       "<tr><td>H2O_cluster_total_cores:</td>\n",
       "<td>6</td></tr>\n",
       "<tr><td>H2O_cluster_allowed_cores:</td>\n",
       "<td>6</td></tr>\n",
       "<tr><td>H2O_cluster_status:</td>\n",
       "<td>accepting new members, healthy</td></tr>\n",
       "<tr><td>H2O_connection_url:</td>\n",
       "<td>http://127.0.0.1:54321</td></tr>\n",
       "<tr><td>H2O_connection_proxy:</td>\n",
       "<td>{\"http\": null, \"https\": null}</td></tr>\n",
       "<tr><td>H2O_internal_security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O_API_Extensions:</td>\n",
       "<td>Amazon S3, Algos, AutoML, Core V3, TargetEncoder, Core V4</td></tr>\n",
       "<tr><td>Python_version:</td>\n",
       "<td>3.7.6 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ---------------------------------------------------------\n",
       "H2O_cluster_uptime:         01 secs\n",
       "H2O_cluster_timezone:       Australia/Sydney\n",
       "H2O_data_parsing_timezone:  UTC\n",
       "H2O_cluster_version:        3.30.0.3\n",
       "H2O_cluster_version_age:    6 days\n",
       "H2O_cluster_name:           H2O_from_python_Jamil_5f9k98\n",
       "H2O_cluster_total_nodes:    1\n",
       "H2O_cluster_free_memory:    1.984 Gb\n",
       "H2O_cluster_total_cores:    6\n",
       "H2O_cluster_allowed_cores:  6\n",
       "H2O_cluster_status:         accepting new members, healthy\n",
       "H2O_connection_url:         http://127.0.0.1:54321\n",
       "H2O_connection_proxy:       {\"http\": null, \"https\": null}\n",
       "H2O_internal_security:      False\n",
       "H2O_API_Extensions:         Amazon S3, Algos, AutoML, Core V3, TargetEncoder, Core V4\n",
       "Python_version:             3.7.6 final\n",
       "--------------------------  ---------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th style=\"text-align: right;\">  date_in_days</th><th style=\"text-align: right;\">        0</th><th style=\"text-align: right;\">        1</th><th style=\"text-align: right;\">        2</th><th style=\"text-align: right;\">       3</th><th style=\"text-align: right;\">        4</th><th style=\"text-align: right;\">  5</th><th style=\"text-align: right;\">  6</th><th style=\"text-align: right;\">  7</th><th style=\"text-align: right;\">        8</th><th style=\"text-align: right;\">        9</th><th style=\"text-align: right;\">       10</th><th style=\"text-align: right;\">       11</th><th style=\"text-align: right;\">       12</th><th style=\"text-align: right;\">      13</th><th style=\"text-align: right;\">  14</th><th style=\"text-align: right;\">      15</th><th style=\"text-align: right;\">       16</th><th style=\"text-align: right;\">      17</th><th style=\"text-align: right;\">  18</th><th style=\"text-align: right;\">       19</th><th style=\"text-align: right;\">       20</th><th style=\"text-align: right;\">       21</th><th style=\"text-align: right;\">       22</th><th style=\"text-align: right;\">       23</th><th style=\"text-align: right;\">       24</th><th style=\"text-align: right;\">       25</th><th style=\"text-align: right;\">       26</th><th style=\"text-align: right;\">       27</th><th style=\"text-align: right;\">       28</th><th style=\"text-align: right;\">       29</th><th style=\"text-align: right;\">  publisher_independent</th><th style=\"text-align: right;\">  publisher_irishtimes</th><th style=\"text-align: right;\">  publisher_rte</th><th>class  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td style=\"text-align: right;\">             1</td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0.172441</td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">  0</td><td style=\"text-align: right;\">  0</td><td style=\"text-align: right;\">  0</td><td style=\"text-align: right;\">0.0823194</td><td style=\"text-align: right;\">0.318479 </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0.0349418</td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0       </td><td style=\"text-align: right;\">   0</td><td style=\"text-align: right;\">0       </td><td style=\"text-align: right;\">0.0173897</td><td style=\"text-align: right;\">0       </td><td style=\"text-align: right;\">   0</td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0.253905 </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0.0232926</td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0.0856273</td><td style=\"text-align: right;\">                      1</td><td style=\"text-align: right;\">                     0</td><td style=\"text-align: right;\">              0</td><td>c0     </td></tr>\n",
       "<tr><td style=\"text-align: right;\">             1</td><td style=\"text-align: right;\">0.0183763</td><td style=\"text-align: right;\">0.197183 </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0       </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">  0</td><td style=\"text-align: right;\">  0</td><td style=\"text-align: right;\">  0</td><td style=\"text-align: right;\">0.0801676</td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0.0353892</td><td style=\"text-align: right;\">0       </td><td style=\"text-align: right;\">   0</td><td style=\"text-align: right;\">0.564642</td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0       </td><td style=\"text-align: right;\">   0</td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0.0470004</td><td style=\"text-align: right;\">0.044255 </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">                      1</td><td style=\"text-align: right;\">                     0</td><td style=\"text-align: right;\">              0</td><td>c2     </td></tr>\n",
       "<tr><td style=\"text-align: right;\">             1</td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0       </td><td style=\"text-align: right;\">0.0455377</td><td style=\"text-align: right;\">  0</td><td style=\"text-align: right;\">  0</td><td style=\"text-align: right;\">  0</td><td style=\"text-align: right;\">0.352749 </td><td style=\"text-align: right;\">0.0840476</td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0       </td><td style=\"text-align: right;\">   0</td><td style=\"text-align: right;\">0       </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0       </td><td style=\"text-align: right;\">   0</td><td style=\"text-align: right;\">0.0472732</td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0.0229077</td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0.44111  </td><td style=\"text-align: right;\">                      1</td><td style=\"text-align: right;\">                     0</td><td style=\"text-align: right;\">              0</td><td>c0     </td></tr>\n",
       "<tr><td style=\"text-align: right;\">             1</td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0.0285712</td><td style=\"text-align: right;\">0       </td><td style=\"text-align: right;\">0.075246 </td><td style=\"text-align: right;\">  0</td><td style=\"text-align: right;\">  0</td><td style=\"text-align: right;\">  0</td><td style=\"text-align: right;\">0.180741 </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0       </td><td style=\"text-align: right;\">   0</td><td style=\"text-align: right;\">0       </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0       </td><td style=\"text-align: right;\">   0</td><td style=\"text-align: right;\">0.0380127</td><td style=\"text-align: right;\">0.0123964</td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0.0715387</td><td style=\"text-align: right;\">0.549619 </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0.0327991</td><td style=\"text-align: right;\">                      1</td><td style=\"text-align: right;\">                     0</td><td style=\"text-align: right;\">              0</td><td>c1     </td></tr>\n",
       "<tr><td style=\"text-align: right;\">             1</td><td style=\"text-align: right;\">0.0545967</td><td style=\"text-align: right;\">0.0684742</td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0       </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">  0</td><td style=\"text-align: right;\">  0</td><td style=\"text-align: right;\">  0</td><td style=\"text-align: right;\">0.226768 </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0.0640619</td><td style=\"text-align: right;\">0.0599077</td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0       </td><td style=\"text-align: right;\">   0</td><td style=\"text-align: right;\">0.106851</td><td style=\"text-align: right;\">0.245072 </td><td style=\"text-align: right;\">0       </td><td style=\"text-align: right;\">   0</td><td style=\"text-align: right;\">0.0376349</td><td style=\"text-align: right;\">0.0398107</td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0.0268288</td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0.0104615</td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0.0465236</td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">                      1</td><td style=\"text-align: right;\">                     0</td><td style=\"text-align: right;\">              0</td><td>c0     </td></tr>\n",
       "<tr><td style=\"text-align: right;\">             1</td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0       </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">  0</td><td style=\"text-align: right;\">  0</td><td style=\"text-align: right;\">  0</td><td style=\"text-align: right;\">0.595213 </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0       </td><td style=\"text-align: right;\">   0</td><td style=\"text-align: right;\">0       </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0       </td><td style=\"text-align: right;\">   0</td><td style=\"text-align: right;\">0.111763 </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0.0555266</td><td style=\"text-align: right;\">0.080454 </td><td style=\"text-align: right;\">0.0525433</td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0.0681247</td><td style=\"text-align: right;\">0.01563  </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0.0149856</td><td style=\"text-align: right;\">                      1</td><td style=\"text-align: right;\">                     0</td><td style=\"text-align: right;\">              0</td><td>c1     </td></tr>\n",
       "<tr><td style=\"text-align: right;\">             1</td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0       </td><td style=\"text-align: right;\">0.053908 </td><td style=\"text-align: right;\">  0</td><td style=\"text-align: right;\">  0</td><td style=\"text-align: right;\">  0</td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0.112804</td><td style=\"text-align: right;\">   0</td><td style=\"text-align: right;\">0.145246</td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0       </td><td style=\"text-align: right;\">   0</td><td style=\"text-align: right;\">0.455874 </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0.0720247</td><td style=\"text-align: right;\">0.08981  </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0.061805 </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">                      0</td><td style=\"text-align: right;\">                     1</td><td style=\"text-align: right;\">              0</td><td>c2     </td></tr>\n",
       "<tr><td style=\"text-align: right;\">             1</td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0       </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">  0</td><td style=\"text-align: right;\">  0</td><td style=\"text-align: right;\">  0</td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0.692612 </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0       </td><td style=\"text-align: right;\">   0</td><td style=\"text-align: right;\">0       </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0       </td><td style=\"text-align: right;\">   0</td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0.215381 </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0.0835987</td><td style=\"text-align: right;\">                      0</td><td style=\"text-align: right;\">                     1</td><td style=\"text-align: right;\">              0</td><td>c0     </td></tr>\n",
       "<tr><td style=\"text-align: right;\">             1</td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0       </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">  0</td><td style=\"text-align: right;\">  0</td><td style=\"text-align: right;\">  0</td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0.586376 </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0.0375144</td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0       </td><td style=\"text-align: right;\">   0</td><td style=\"text-align: right;\">0       </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0       </td><td style=\"text-align: right;\">   0</td><td style=\"text-align: right;\">0.0510444</td><td style=\"text-align: right;\">0.0256627</td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0.273435 </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0.0158067</td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">                      0</td><td style=\"text-align: right;\">                     1</td><td style=\"text-align: right;\">              0</td><td>c2     </td></tr>\n",
       "<tr><td style=\"text-align: right;\">             1</td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0       </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">  0</td><td style=\"text-align: right;\">  0</td><td style=\"text-align: right;\">  0</td><td style=\"text-align: right;\">0.114227 </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0.0659834</td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0.220251 </td><td style=\"text-align: right;\">0       </td><td style=\"text-align: right;\">   0</td><td style=\"text-align: right;\">0       </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0.156956</td><td style=\"text-align: right;\">   0</td><td style=\"text-align: right;\">0.0167668</td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0.129265 </td><td style=\"text-align: right;\">0.0609422</td><td style=\"text-align: right;\">0.0340686</td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">0.165624 </td><td style=\"text-align: right;\">0.0150207</td><td style=\"text-align: right;\">0        </td><td style=\"text-align: right;\">                      0</td><td style=\"text-align: right;\">                     1</td><td style=\"text-align: right;\">              0</td><td>c1     </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# H2O Testing\n",
    "h2o.init()\n",
    "str_labels = [\"c\" + str(x) for x in labels]\n",
    "combined = pd.concat([features, pd.DataFrame(str_labels, columns=[\"class\"]).astype(str)], axis=1)\n",
    "h2o_dataset = h2o.H2OFrame(combined)\n",
    "h2o_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = h2o_dataset.columns[:-1]\n",
    "y = h2o_dataset.columns[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Iteration 0\n",
      "> Iteration 1\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "AutoML progress: |\n",
      "11:19:39.435: AutoML: XGBoost is not available; skipping it.\n",
      "\n",
      "███████████████████████████████████████████████████████"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "f1 = []\n",
    "counter = 0\n",
    "\n",
    "for train_ind, val_ind in kf.split(h2o_dataset):\n",
    "    print(\"> Iteration\", counter)\n",
    "    if counter != 0:\n",
    "        train = h2o.H2OFrame(combined.iloc[train_ind])\n",
    "        val = h2o.H2OFrame(combined.iloc[val_ind])\n",
    "        val_labels = combined.iloc[val_ind]['class']\n",
    "\n",
    "        aml = H2OAutoML(max_runtime_secs=3600, seed=1)\n",
    "        aml.train(x=x,y=y, training_frame=train)\n",
    "        pred = aml.leader.predict(val)\n",
    "        pred = h2o.as_list(pred[:, 0])\n",
    "\n",
    "        acc.append(accuracy_score(pred, val_labels))\n",
    "        f1.append(f1_score(pred, val_labels, average='macro'))\n",
    "        \n",
    "    counter += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Testing\n",
    "acc = []\n",
    "f1 = []\n",
    "\n",
    "for train_ind, val_ind in kf.split(features, labels):\n",
    "    X_train, y_train = features.iloc[train_ind], labels[train_ind]\n",
    "    X_val, y_val = features.iloc[val_ind], labels[val_ind]\n",
    "    \n",
    "    svm = SVC(kernel='linear')\n",
    "    svm.fit(X_train, y_train)\n",
    "    \n",
    "    acc.append(SCORERS['accuracy'](svm, X_val, y_val))\n",
    "    f1.append(SCORERS['f1_macro'](svm, X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Testing\n",
    "acc = []\n",
    "f1 = []\n",
    "\n",
    "for train_ind, val_ind in kf.split(features, labels):\n",
    "    X_train, y_train = features.iloc[train_ind], labels[train_ind]\n",
    "    X_val, y_val = features.iloc[val_ind], labels[val_ind]\n",
    "    \n",
    "    rf = RandomForestClassifier()\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    acc.append(SCORERS['accuracy'](rf, X_val, y_val))\n",
    "    f1.append(SCORERS['f1_macro'](rf, X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}