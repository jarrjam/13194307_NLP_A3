{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gensim\n",
    "import nltk\n",
    "import pickle\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "nltk.download('stopwords')\n",
    "from tpot import TPOTClassifier\n",
    "from datetime import datetime\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import SCORERS, accuracy_score, f1_score\n",
    "import h2o\n",
    "from h2o.automl import H2OAutoML\n",
    "\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "DATE_FORMAT = \"%Y-%m-%d\"\n",
    "MIN_DATE = datetime.strptime('2009-07-07', DATE_FORMAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_terms(terms_file):\n",
    "    term_dict = {}\n",
    "    \n",
    "    for line in terms_file:\n",
    "        key, word = line.split(\",\")\n",
    "        term_dict[int(key)] = word.split(\"\\n\")[0]\n",
    "        \n",
    "    return term_dict\n",
    "\n",
    "def get_labels(labels_file):\n",
    "    curr_article = -1\n",
    "    labels_votes = []\n",
    "    article_dict = {}\n",
    "\n",
    "    for line in labels_file:\n",
    "        split_line = line.split(\",\")\n",
    "\n",
    "        if int(split_line[0]) != curr_article:\n",
    "            curr_article = int(split_line[0])\n",
    "            article_dict[curr_article] = True\n",
    "            labels_votes.append([0, 0, 0]) # index 0 = negative, 1 = irrelevant, 2 = positive\n",
    "\n",
    "        if 'negative' in split_line[2]:\n",
    "            labels_votes[-1][0] += 1\n",
    "        elif 'irrelevant' in split_line[2]:\n",
    "            labels_votes[-1][1] += 1\n",
    "        else:\n",
    "            labels_votes[-1][2] += 1\n",
    "\n",
    "    labels = []\n",
    "\n",
    "    for vote_counts in labels_votes:\n",
    "        labels.append(np.argmax(vote_counts))\n",
    "        \n",
    "    return labels, article_dict\n",
    "\n",
    "def create_bow_and_features(word_list, term_dict):\n",
    "    word_list = word_list.split(\",\")\n",
    "    \n",
    "    doc = []\n",
    "    \n",
    "    for i, entry in enumerate(word_list):\n",
    "        if i > 2:\n",
    "            word, freq = entry.split(\":\")\n",
    "\n",
    "            if term_dict[int(word)] not in stop_words:\n",
    "                doc.append((int(word), int(freq)))\n",
    "            \n",
    "    curr_date = datetime.strptime(word_list[1], DATE_FORMAT)\n",
    "    delta = curr_date - MIN_DATE\n",
    "    return doc, word_list[0], delta.days, word_list[2]\n",
    "\n",
    "def preprocess_datasets(articles_dir, terms_dir, annotations_dir, preload_lda=True):\n",
    "    terms_file = open(terms_dir)\n",
    "    term_dict = get_terms(terms_file)\n",
    "    \n",
    "    dataset = []\n",
    "    features = []\n",
    "    articles_file = open(articles_dir)\n",
    "\n",
    "    for line in articles_file:\n",
    "        article, article_id, date_in_days, provider = create_bow_and_features(line, term_dict)\n",
    "        dataset.append(article)\n",
    "        features.append([article_id, provider, date_in_days])\n",
    "\n",
    "    features = pd.DataFrame(features)\n",
    "    \n",
    "    ldamodel = None\n",
    "    num_topics = 30\n",
    "    \n",
    "    if preload_lda is False:\n",
    "        ldamodel = gensim.models.ldamodel.LdaModel(dataset, num_topics = num_topics, passes=15)\n",
    "        ldamodel.save('lda/irish_sentiment/model5.gensim')\n",
    "    else:\n",
    "        ldamodel =  gensim.models.ldamodel.LdaModel.load('lda/irish_sentiment/model5.gensim')\n",
    "        \n",
    "    topic_features = []\n",
    "\n",
    "    for doc in dataset:\n",
    "        row = [0.0 for i in range(num_topics)]\n",
    "\n",
    "        for topic, prob in ldamodel[doc]:\n",
    "            row[topic] = prob\n",
    "\n",
    "        topic_features.append(row)\n",
    "\n",
    "    topic_features = pd.DataFrame(topic_features)\n",
    "    \n",
    "    features = pd.concat([features, topic_features], axis=1)\n",
    "\n",
    "    columns = ['id', 'publisher', 'date_in_days']\n",
    "    topic_columns = [i for i in range(num_topics)]\n",
    "    columns = np.concatenate([columns, topic_columns])\n",
    "    features.columns = columns\n",
    "    \n",
    "    labels_file = open(annotations_dir)\n",
    "    labels, article_dict = get_labels(labels_file)\n",
    "    \n",
    "    drop_list = []\n",
    "\n",
    "    for index, row, in features.iterrows():\n",
    "        if int(row['id']) not in article_dict:\n",
    "            drop_list.append(index)\n",
    "            \n",
    "    features = features.drop(drop_list, axis=0)\n",
    "    features = features.drop('id', axis=1)\n",
    "    features = pd.get_dummies(features)\n",
    "    features = features.reset_index()\n",
    "    features = features.drop('index', axis=1)\n",
    "    \n",
    "    return features, np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_dir = r'datasets\\irish_sentiment\\sentiment_all_terms.csv'\n",
    "articles_dir = r'datasets\\irish_sentiment\\sentiment_all_articles.csv'\n",
    "annotations_dir = r'datasets\\irish_sentiment\\sentiment_all_annotations.csv'\n",
    "\n",
    "features, labels = preprocess_datasets(articles_dir, terms_dir, annotations_dir, preload_lda=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TPOT Testing\n",
    "kf = KFold(5, shuffle=True, random_state=42)\n",
    "\n",
    "acc = []\n",
    "f1 = []\n",
    "\n",
    "for train_ind, val_ind in kf.split(features, labels):\n",
    "    X_train, y_train = features.iloc[train_ind], labels[train_ind]\n",
    "    X_val, y_val = features.iloc[val_ind], labels[val_ind]\n",
    "    \n",
    "    tpot = TPOTClassifier(max_time_mins=60, verbosity=2)\n",
    "    tpot.fit(X_train, y_train)\n",
    "    clf = tpot.fitted_pipeline_\n",
    "    \n",
    "    \n",
    "    acc.append(SCORERS['accuracy'](clf, X_val, y_val))\n",
    "    f1.append(SCORERS['f1_macro'](clf, X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(acc)\n",
    "print(f1)\n",
    "print(neg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H2O Testing\n",
    "h2o.init()\n",
    "str_labels = [\"c\" + str(x) for x in labels]\n",
    "combined = pd.concat([features, pd.DataFrame(str_labels, columns=[\"class\"]).astype(str)], axis=1)\n",
    "h2o_dataset = h2o.H2OFrame(combined)\n",
    "h2o_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = h2o_dataset.columns[:-1]\n",
    "y = h2o_dataset.columns[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = []\n",
    "f1 = []\n",
    "counter = 0\n",
    "\n",
    "for train_ind, val_ind in kf.split(h2o_dataset):\n",
    "    print(\"> Iteration\", counter)\n",
    "    if counter != 0:\n",
    "        train = h2o.H2OFrame(combined.iloc[train_ind])\n",
    "        val = h2o.H2OFrame(combined.iloc[val_ind])\n",
    "        val_labels = combined.iloc[val_ind]['class']\n",
    "\n",
    "        aml = H2OAutoML(max_runtime_secs=3600, seed=1)\n",
    "        aml.train(x=x,y=y, training_frame=train)\n",
    "        pred = aml.leader.predict(val)\n",
    "        pred = h2o.as_list(pred[:, 0])\n",
    "\n",
    "        acc.append(accuracy_score(pred, val_labels))\n",
    "        f1.append(f1_score(pred, val_labels, average='macro'))\n",
    "        \n",
    "    counter += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Testing\n",
    "acc = []\n",
    "f1 = []\n",
    "\n",
    "for train_ind, val_ind in kf.split(features, labels):\n",
    "    X_train, y_train = features.iloc[train_ind], labels[train_ind]\n",
    "    X_val, y_val = features.iloc[val_ind], labels[val_ind]\n",
    "    \n",
    "    svm = SVC(kernel='linear')\n",
    "    svm.fit(X_train, y_train)\n",
    "    \n",
    "    acc.append(SCORERS['accuracy'](svm, X_val, y_val))\n",
    "    f1.append(SCORERS['f1_macro'](svm, X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Testing\n",
    "acc = []\n",
    "f1 = []\n",
    "\n",
    "for train_ind, val_ind in kf.split(features, labels):\n",
    "    X_train, y_train = features.iloc[train_ind], labels[train_ind]\n",
    "    X_val, y_val = features.iloc[val_ind], labels[val_ind]\n",
    "    \n",
    "    rf = RandomForestClassifier()\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    acc.append(SCORERS['accuracy'](rf, X_val, y_val))\n",
    "    f1.append(SCORERS['f1_macro'](rf, X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
