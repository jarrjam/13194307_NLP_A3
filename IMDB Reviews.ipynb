{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gensim\n",
    "import spacy\n",
    "import nltk\n",
    "import pickle\n",
    "from gensim import corpora\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "from spacy.lang.en import English\n",
    "from tpot import TPOTClassifier\n",
    "from datetime import datetime\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import SCORERS, accuracy_score, f1_score\n",
    "import h2o\n",
    "from h2o.automl import H2OAutoML\n",
    "\n",
    "parser = English()\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "DATE_FORMAT = \"%Y-%m-%d\"\n",
    "MIN_DATE = datetime.strptime('2009-07-07', DATE_FORMAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    lda_tokens = []\n",
    "    tokens = parser(text)\n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        elif token.like_url:\n",
    "            lda_tokens.append('URL')\n",
    "        elif token.orth_.startswith('@'):\n",
    "            lda_tokens.append('SCREEN_NAME')\n",
    "        else:\n",
    "            lda_tokens.append(token.lower_)\n",
    "    return lda_tokens\n",
    "\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma\n",
    "    \n",
    "def prepare_text_for_lda(text):\n",
    "    tokens = tokenize(text)\n",
    "    tokens = [token for token in tokens if len(token) > 4]\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "def preprocess_dataset(dataset_dir, load_vocab_and_lda=True):\n",
    "    dataset = pd.read_csv(dataset_dir)\n",
    "    features = dataset['text']\n",
    "    labels = dataset['sentiment']\n",
    "    \n",
    "    tokenized_samples = []\n",
    "\n",
    "    for row in features:\n",
    "        tokenized_samples.append(prepare_text_for_lda(row))\n",
    "    \n",
    "    if load_vocab_and_lda:\n",
    "        vocabulary = pickle.load(open('lda/imdb_reviews/vocab.pkl', 'rb'))\n",
    "    else:\n",
    "        vocabulary = corpora.Dictionary(tokenized_samples)\n",
    "        pickle.dump(vocabulary, open('lda/imdb_reviews/vocab.pkl', 'wb'))\n",
    "        \n",
    "    corpus = [vocabulary.doc2bow(doc) for doc in tokenized_samples]\n",
    "    num_topics = 30\n",
    "    \n",
    "    if load_vocab_and_lda:\n",
    "        ldamodel =  gensim.models.ldamodel.LdaModel.load('lda/imdb_reviews/model5.gensim')\n",
    "    else:\n",
    "        ldamodel = gensim.models.ldamodel.LdaModel(corpus, id2word=vocabulary, num_topics = num_topics, passes=15)\n",
    "        ldamodel.save('lda/imdb_reviews/model5.gensim')\n",
    "        \n",
    "    topic_features = []\n",
    "\n",
    "    for doc in corpus:\n",
    "        row = [0.0 for i in range(num_topics)]\n",
    "\n",
    "        for topic, prob in ldamodel[doc]:\n",
    "            row[topic] = prob\n",
    "\n",
    "        topic_features.append(row)\n",
    "\n",
    "    topic_features = pd.DataFrame(topic_features)\n",
    "    \n",
    "    return topic_features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = 'datasets/imdb_reviews/train.csv'\n",
    "\n",
    "features, labels = preprocess_dataset(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H2O Testing\n",
    "h2o.init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_labels = [\"c\" + str(x) for x in labels]\n",
    "combined = pd.concat([features, pd.DataFrame(str_labels, columns=[\"class\"]).astype(str)], axis=1)\n",
    "h2o_dataset = h2o.H2OFrame(combined)\n",
    "h2o_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = h2o_dataset.columns[:-1]\n",
    "y = h2o_dataset.columns[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(5, shuffle=True, random_state=42)\n",
    "\n",
    "acc = []\n",
    "f1 = []\n",
    "counter = 0\n",
    "\n",
    "for train_ind, val_ind in kf.split(h2o_dataset):\n",
    "    print(\"> Iteration\", counter)\n",
    "    train = h2o.H2OFrame(combined.iloc[train_ind])\n",
    "    val = h2o.H2OFrame(combined.iloc[val_ind])\n",
    "    val_labels = combined.iloc[val_ind]['class']\n",
    "\n",
    "    aml = H2OAutoML(max_runtime_secs=3600, seed=1)\n",
    "    aml.train(x=x,y=y, training_frame=train)\n",
    "    pred = aml.leader.predict(val)\n",
    "    pred = h2o.as_list(pred[:, 0])\n",
    "\n",
    "    acc.append(accuracy_score(pred, val_labels))\n",
    "    f1.append(f1_score(pred, val_labels, average='macro'))\n",
    "        \n",
    "    counter += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TPOT Testing\n",
    "acc = []\n",
    "f1 = []\n",
    "\n",
    "for train_ind, val_ind in kf.split(features, labels):\n",
    "    X_train, y_train = features.iloc[train_ind], labels[train_ind]\n",
    "    X_val, y_val = features.iloc[val_ind], labels[val_ind]\n",
    "    \n",
    "    tpot = TPOTClassifier(max_time_mins=60, verbosity=2)\n",
    "    tpot.fit(X_train, y_train)\n",
    "    clf = tpot.fitted_pipeline_\n",
    "    \n",
    "    \n",
    "    acc.append(SCORERS['accuracy'](clf, X_val, y_val))\n",
    "    f1.append(SCORERS['f1_macro'](clf, X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Testing\n",
    "acc = []\n",
    "f1 = []\n",
    "\n",
    "for train_ind, val_ind in kf.split(features, labels):\n",
    "    X_train, y_train = features.iloc[train_ind], labels[train_ind]\n",
    "    X_val, y_val = features.iloc[val_ind], labels[val_ind]\n",
    "    \n",
    "    svm = SVC(kernel='linear')\n",
    "    svm.fit(X_train, y_train)\n",
    "    \n",
    "    acc.append(SCORERS['accuracy'](svm, X_val, y_val))\n",
    "    f1.append(SCORERS['f1_macro'](svm, X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Testing\n",
    "acc = []\n",
    "f1 = []\n",
    "\n",
    "for train_ind, val_ind in kf.split(features, labels):\n",
    "    X_train, y_train = features.iloc[train_ind], labels[train_ind]\n",
    "    X_val, y_val = features.iloc[val_ind], labels[val_ind]\n",
    "    \n",
    "    rf = RandomForestClassifier()\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    acc.append(SCORERS['accuracy'](rf, X_val, y_val))\n",
    "    f1.append(SCORERS['f1_macro'](rf, X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
